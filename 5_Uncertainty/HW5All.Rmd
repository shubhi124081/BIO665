---
title: "Homework 5 Responses"
author: "Zeyi Han, Shubhi Sharma, Margaret Swift"
date: "2/24/2020"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=F}
#----------------------------------------------------------------------------
# LOADING
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, cache=TRUE)
pacman::p_load(ggplot2)
source('../clarkfunctions2020.R')
# Rcpp::sourceCpp('../cppFns.cpp')
# any libraries and data here

#----------------------------------------------------------------------------
# FUNCTIONS
# put functions here

```


## Exercise 1 {#q1}
Find the MLE, the likelihood profile, and the 95% CI for waiting times that are 
exponentially distributed:

$$
L(\mathbf{y}; \lambda) = \prod_{i=1}^n \lambda e^{-y_i \lambda}
$$

```{r q1}
#----------------------------------------------------------------------------
## EXERCISE 1

```

## Exercise 2 {#q2}
Use Fisher Information to find the standard error of the mean of a normal 
sampling distribution.  

$Solution:$ First, we find the likelihood function and only take the proportional
 pieces with respect to $\mu$:
$$
L(\mathbf{y};\mu, \sigma^2) 
= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac12 \frac{(y_i-\mu)^2}{\sigma^2}\right\}
\propto\exp\left\{-\frac12 \frac{\sum (y_i-\mu)^2}{\sigma^2}\right\}
$$
Next, we take the log of the likelihood, and find the second derivative with respect to $\mu$.
$$
\log L(\mathbf{y};\mu, \sigma^2) = 
-\frac{1}{2\sigma^2} \sum y_i^2-2y_i\mu + \mu^2
$$

$$
\frac{d}{d\mu}\left[\log L(\mathbf{y};\mu, \sigma^2)\right] = 
\sum \frac{y_i}{\sigma^2} - \frac{\mu}{\sigma^2}
$$

$$
\frac{d^2}{d\mu^2}\left[\log L(\mathbf{y};\mu, \sigma^2)\right] = 
- \frac{n}{\sigma^2}
$$

$$
I = \frac{n}{\sigma^2}\bigg\rvert_{\hat{\mu}} = \frac{n}{\sigma^2}
$$

Now that we have the Fisher's Information, we can use it to estimate the standard error of the mean:
$$
se_{\hat{\mu}} = \frac{1}{\sqrt{I}} = \frac{\sigma}{\sqrt n}
$$

The standard error of the mean will decrease with our sample size $n$ as $n\to\infty$.

## Exercise 3 {#q3}
Estimate the standard error for the exponential model using Fisher Information.  

$Solution:$ First, we find the likelihood function and only take the proportional
 pieces w.r.t. $\lambda$:

$$
L(\mathbf{y};\lambda) 
= \prod_{i=1}^n \lambda e^{-\lambda y_i} = \lambda^ne^{-\lambda\sum y_i}
$$
Next, we take the log of the likelihood, and find the second derivative w.r.t. $\lambda$.
$$
\log L(\mathbf{y};\lambda) 
 = n\log\lambda -\lambda\sum y_i
$$
We take the first derivative, and stop to find the MLE for $\lambda$:
$$
\frac{d}{d\lambda}\left[\log L(\mathbf{y};\lambda) \right]
 = \frac{n}{\lambda} - \sum y_i
\implies
\hat{\lambda} = \frac{n}{\sum y_i}
$$
And continue to find the second derivative for Fisher's:
$$
\frac{d^2}{d\mu^2}\left[\log L(\mathbf{y};\lambda) \right]
 = -\frac{n}{\lambda^2}
$$
$$
I = \frac{n}{\lambda^2}\bigg\rvert_{\hat{\lambda}} 
= \frac{n}{\left(n/\sum y_i\right)^2}
= \frac{\left(\sum y_i\right)^2}{n}
$$

Now that we have the Fisher's Information, we can use it to estimate the standard error:
$$
se_{\hat{\lambda}} 
= \frac{1}{\sqrt{I}} 
=\sqrt{\frac{n}{\left(\sum y_i\right)^2}} = \frac{\sqrt{n}}{\sum y_i}
$$

The standard error will still decrease as our sample size increases to $\infty$, 
because we have a sum of our $y_i$ on the bottom, leading to an $O(\sqrt{n} / n )$
behavior, which is stronger in the denominator for $y_i > 1$.


## Exercise 4 {#q4}
For the cone example, I used the likelihood $Poi(y_i | \beta x_i)$.  Combine this 
likelihood with the prior $gamma(\beta | a, b)$ and answer the following:

a) What is the posterior density for $\beta$?

b) For simulated data sets of $n = 5$, how do the standard errors and credible 
intervals for this model compare with Fisher information?  

c) The form of the Bayesian standard error and the standard error from Fisher 
Information look different.  Can you explain why numerically they are similar? 
[Hint: think about sample size $n$].


```{r q4}
#----------------------------------------------------------------------------
## EXERCISE 4

```

## Group exercise

I want to get a feel for the uncertainty on the estimate of the variance for a 
Gaussian model for continuous observations $y$.  I would like to compare 
different methods.

1.  Find the MLE for the variance.  If you have time to kill, derive the Fisher 
information and the SE. Compare the estimates for different sample sizes.

```{r group}
#----------------------------------------------------------------------------
## GROUP EXERCISE

MLE <- function(n) {
  mu <- 0; 
  y <- rnorm(n, mu)
  return(1/n * sum((y-mu)^2))
}
```

To find the MLE, we have an approximate Gaussian density:

$$
(2\pi\sigma^2)^{-n/2}\exp\left\{ -\frac12 \frac{(\sum y_i-\mu)^2}{\sigma^2}\right\}
\propto 
(\sigma^2)^{-n/2}\exp\left\{ -\frac12 \frac{(\sum y_i-\mu)^2}{\sigma^2}\right\}
$$
took the log, found the first derivative, and set it equal to zero:

$$
\frac{d}{d\sigma^2}\left[-\frac{n}{2}\log(\sigma^2) -\frac12 (\sum y_i-\mu)^2(\sigma^2)^{-1}\right]
= -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\left(\sum y_i-\mu\right)^2 = 0
$$

$$
\frac{n}{2\sigma^2} = \frac{1}{2(\sigma^2)^2}\left(\sum y_i-\mu\right)^2
$$
$$
\sigma^2_{MLE} = \frac{1}{n}\left(\sum y_i-\mu\right)^2
$$

The MLEs we found for variance for $n=1, 10, 1000$ are, respectively, 
`r round(MLE(1),4)`, `r round(MLE(10),4)`, and `r round(MLE(1000),4)`. As you can
see, the variance comes closer and closer to the true variance, 1, as the sample size 
increases.

2. Generate bootstrapped estimates for the uncertainty.

3. Complete a Bayesian analysis, choose a prior. Derive the result, then use MCMC.



## Appendix {#appendix}

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```


