---
title: "Homework 5 Responses"
author: "Zeyi Han, Shubhi Sharma, Margaret Swift"
date: "2/24/2020"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
header-includes: 
- \usepackage{placeins}
- \usepackage{color}
- \usepackage{amsmath, amssymb}
- \DeclareMathOperator{\E}{\mathbb{E}}
---

```{r setup, include=F}
#----------------------------------------------------------------------------
# LOADING
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, cache=TRUE)
pacman::p_load(ggplot2)
source('../clarkfunctions2020.R')
# Rcpp::sourceCpp('../cppFns.cpp')
# any libraries and data here

#----------------------------------------------------------------------------
# FUNCTIONS
# put functions here

```

## Exercise 1 {#q1}
Find the MLE, the likelihood profile, and the 95% CI for waiting times that are exponentially distributed:

$$
L(\mathbf{y}; \lambda) = \prod_{i=1}^n \lambda e^{-y_i \lambda}
$$

-------


Log likelihood of exponential distribution is:

$$
\log L(\mathbf{y}; \lambda) =  n\log\lambda - n\lambda\bar{y}
$$

Solving the partial derivative of the log likelihood with respect to $\lambda$, we get the MLE of exponential distribution:

$$
\hat{\lambda} = \frac{1}{\bar{y}}
$$

The likelihood ratio is:

$$
\log R =  \log \lambda_0 + \log \bar{y}  -\bar{y} \lambda_0 + 1
$$

Here is a simulated data set and its MLE, likelihood profile and 95% CI.

```{r}

#----------------------------------------------------------------------------
## EXERCISE 1

n    <- 1000
y    <- rexp(n)                                   #simulated waiting times
ybar <- mean(y)
ycap <- 1/ybar
rseq <- seq(0,5,length=100)*ybar
R <- log(rseq) + log(ybar) - ybar * rseq + 1   #normed likelihood

par(mfrow=c(3,1), mar=c(4,4,1,4))
plot(rseq,R, type = "l", xlab = "")
abline(v=ycap,lty=2)
D  <- -2*R                # Deviance
plot(rseq,D,type='l', xlab='') 
abline(v=ycap,lty=2)
abline(h=3.84,lty=2)      # value of D at P = 0.05

P <- 1 - pchisq(D,1)      #P value
plot(rseq, P, type='l', xlab='lambda_0') 
abline(v=ycap,lty=2)
abline(h=.05, lty=2)

#find the 95% CI:
CI <- range(rseq[P > .05])
abline(v=CI, lty=2, col = "blue")
```

## Exercise 2 {#q2}
Use Fisher Information to find the standard error of the mean of a normal 
sampling distribution.  

$Solution:$ First, we find the likelihood function and only take the proportional
 pieces with respect to $\mu$:
$$
L(\mathbf{y};\mu, \sigma^2) 
= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac12 \frac{(y_i-\mu)^2}{\sigma^2}\right\}
\propto\exp\left\{-\frac12 \frac{\sum (y_i-\mu)^2}{\sigma^2}\right\}
$$
Next, we take the log of the likelihood, and find the second derivative with respect to $\mu$.
$$
\log L(\mathbf{y};\mu, \sigma^2) = 
-\frac{1}{2\sigma^2} \sum y_i^2-2y_i\mu + \mu^2
$$

$$
\frac{d}{d\mu}\left[\log L(\mathbf{y};\mu, \sigma^2)\right] = 
\sum \frac{y_i}{\sigma^2} - \frac{\mu}{\sigma^2}
$$

$$
\frac{d^2}{d\mu^2}\left[\log L(\mathbf{y};\mu, \sigma^2)\right] = 
- \frac{n}{\sigma^2}
$$

$$
I = \frac{n}{\sigma^2}\bigg\rvert_{\hat{\mu}} = \frac{n}{\sigma^2}
$$

Now that we have the Fisher's Information, we can use it to estimate the standard error of the mean:
$$
se_{\hat{\mu}} = \frac{1}{\sqrt{I}} = \frac{\sigma}{\sqrt n}
$$

The standard error of the mean will decrease with our sample size $n$ as $n\to\infty$.

## Exercise 3 {#q3}
Estimate the standard error for the exponential model using Fisher Information.  

$Solution:$ First, we find the likelihood function and only take the proportional
 pieces w.r.t. $\lambda$:

$$
L(\mathbf{y};\lambda) 
= \prod_{i=1}^n \lambda e^{-\lambda y_i} = \lambda^ne^{-\lambda\sum y_i}
$$
Next, we take the log of the likelihood, and find the second derivative w.r.t. $\lambda$.
$$
\log L(\mathbf{y};\lambda) 
 = n\log\lambda - n\lambda\bar{y}
$$
We take the first derivative, and stop to find the MLE for $\lambda$:
$$
\frac{d}{d\lambda}\left[\log L(\mathbf{y};\lambda) \right]
 = \frac{n}{\lambda} - n\bar{y}
\implies
\hat{\lambda} = \frac{1}{\bar{y}}
$$
And continue to find the second derivative for Fisher's:
$$
\frac{d^2}{d\mu^2}\left[\log L(\mathbf{y};\lambda) \right]
 = -\frac{n}{\lambda^2}
$$
$$
I = \frac{n}{\lambda^2}\bigg\rvert_{\hat{\lambda}} 
= \frac{n}{\left(1/\bar{y}\right)^2}
= n\bar{y}^2
$$

Now that we have the Fisher's Information, we can use it to estimate the standard error:
$$
se_{\hat{\lambda}} 
= \frac{1}{\sqrt{I}} 
=\sqrt{\frac{1}{n\bar{y}^2}}
$$

The standard error will still decrease as our sample size increases to $\infty$, 
for $y_i>1$.


## Exercise 4 {#q4}
For the cone example, I used the likelihood $Poi(y_i | \beta x_i)$.  Combine this 
likelihood with the prior $gamma(\beta | a, b)$ and answer the following:

a) What is the posterior density for $\beta$?

b) For simulated data sets of $n = 5$, how do the standard errors and credible 
intervals for this model compare with Fisher information?  

c) The form of the Bayesian standard error and the standard error from Fisher 
Information look different.  Can you explain why numerically they are similar? 
[Hint: think about sample size $n$].


```{r q4}
#----------------------------------------------------------------------------
## EXERCISE 4

```

## Group exercise

I want to get a feel for the uncertainty on the estimate of the variance for a 
Gaussian model for continuous observations $y$.  I would like to compare 
different methods.

1.  Find the MLE for the variance.  If you have time to kill, derive the Fisher 
information and the SE. Compare the estimates for different sample sizes.

```{r group}
#----------------------------------------------------------------------------
## GROUP EXERCISE

MLE <- function(n) {
  mu <- 0; 
  y <- rnorm(n, mu)
  return(1/n * sum((y-mu)^2))
}
```

To find the MLE, we have an approximate Gaussian density:

$$
(2\pi\sigma^2)^{-n/2}\exp\left\{ -\frac12 \frac{(\sum y_i-\mu)^2}{\sigma^2}\right\}
\propto 
(\sigma^2)^{-n/2}\exp\left\{ -\frac12 \frac{(\sum y_i-\mu)^2}{\sigma^2}\right\}
$$
took the log, found the first derivative, and set it equal to zero:

$$
\frac{d}{d\sigma^2}\left[-\frac{n}{2}\log(\sigma^2) -\frac12 (\sum y_i-\mu)^2(\sigma^2)^{-1}\right]
= -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\left(\sum y_i-\mu\right)^2 = 0
$$

$$
\frac{n}{2\sigma^2} = \frac{1}{2(\sigma^2)^2}\left(\sum y_i-\mu\right)^2
$$
$$
\sigma^2_{MLE} = \frac{1}{n}\left(\sum y_i-\mu\right)^2
$$

The MLEs we found for variance for $n=1, 10, 1000$ are, respectively, 
`r round(MLE(1),4)`, `r round(MLE(10),4)`, and `r round(MLE(1000),4)`. As you can
see, the variance comes closer and closer to the true variance, 1, as the sample size 
increases.


## Group exercise 

MLE for variance 

Let $y_i \sim N(0, \sigma^2)$ and $\E[x] = 0$

$$
\begin{aligned}
p(Y) &= \left( \frac{1}{2\pi \sigma^2} \right)^{-n/2} e\left(\frac{-1}{2\sigma^2}\sum_i(y_i)^2 \right)\\
\text{Let } \sigma^2 &= \theta\\
\log L(\theta \mid Y) &= \frac{-n}{2}\left(\log \theta + \frac{1}{\theta} \frac{\sum y_i^2}{n}\right)\\
\frac{\partial}{\partial \theta}&= \frac{-n}{2}\left( \theta - \frac{1}{\theta^2} \frac{\sum y_i^2}{n}\right)\\
\text{Setting first } & \text{derivarite to 0, }\\
0 &= \frac{-n}{2}\left( \theta - \frac{1}{\theta^2} \frac{\sum y_i^2}{n}\right)\\
\hat{\theta} &= \frac{\sum y_i^2}{n}
\end{aligned}
$$


Bootstrap to get MLE estimate for variance 

```{r}
set.seed(123)
y <- rnorm(20, 0, 1)
Y <- mean(y)
nlo <- length(y)

nboot <- 2000
varVals <- matrix(0, nboot, 1)

for(b in 1:nboot){
  
  bindex <- sample(y, nlo, replace = T)
  Y <- mean(bindex)
  varVals[b,1] <- sum(bindex^2)/nlo

}

hist(varVals[,1], main = paste0("Estimates of variance"), xlab = "Estimates")


#sample variance 

varValsSample <- matrix(0, nboot, 1)

for(b in 1:nboot){
  
    bindex <- sample(y, nlo, replace = T)
  Y <- mean(bindex)
  div <- 1/(nlo - 1)
  varValsSample[b,1] <- sum((bindex - Y)^2)*div
}

hist(varValsSample[,1], main = paste0("Estimates of sample variance"), xlab = "Estimates")
#this does better 
```



Bayesian version 
Writing the variance as precision
$$
\begin{aligned}
p(y \mid \theta ) &=  \left( \frac{\theta}{2\pi} \right)^{-n/2} e\left(\frac{-\theta}{2}\sum_i(y_i)^2 \right)\\
p(\theta) &\propto \theta^{\alpha -1} e^{-\theta \beta}\\
p(\theta \mid y) &\propto \theta^{\alpha + n/2 - 1} e^{\beta + 0.5 \sum_i (y_i)^2}\\
&\sim Ga(a + n/2, \beta + 0.5 \sum_i(y_i)^2)\\
&\sim Ga(a_n, b_n)\\
\end{aligned}
$$

Sample from the posterior 

```{r}

set.seed(123)
y <- rnorm(20, 0, 1)
n <- length(y)
#hyperparameters 
a <- b <- 0.01

#posterior parameters 
an <- a + n/2
bn <- b + 0.5*sum(y)^2

nsamp <- 2000
varValsBayes <- matrix(0, nsamp,1 )

varValsBayes[, 1] <- rgamma(nsamp, an, bn) 

hist(varValsBayes[,1])
```


## Appendix {#appendix}

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```


