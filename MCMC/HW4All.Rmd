---
title: "Homework 4 Responses"
author: "Zeyi Han, Shubhi Sharma, Margaret Swift"
date: "2/24/2020"
output: 
  prettydoc::html_pretty:
  theme: architect
---

```{r setup, include=FALSE}
#----------------------------------------------------------------------------
# LOADING
knitr::opts_chunk$set(echo = FALSE, eval=TRUE)
library(ggplot2)
library(grid)
library(gridExtra)
library(reshape2)
library(gtable)
library(rjags)
source('../clarkfunctions2020.R')
Rcpp::sourceCpp('../cppFns.cpp')
treedata <- read.table('FACEtrees.txt',header=T)

#----------------------------------------------------------------------------
# FUNCTIONS
len <- function(k) length(k)
getMu <- function(sigma, y, M, m){ # conditional distributions
  n <- length(y)
  V <- 1/( n/sigma + 1/M )
  v <- sum(y)/sigma + m/M
  rnorm(1, V*v, sqrt(V))
}
getSigma <- function(mu, y, s1, s2){
  n  <- length(y)
  u1 <- s1 + n/2
  u2 <- s2 + 1/2*sum( (y - mu)^2 )
  1/rgamma(1, u1, u2)
}
plotFn <- function(normal = str2, trunNorm = bgibbs, ng = 1e4, burnin = NULL){
  par(mfrow = c(3, 2))
  if(is.null(burnin)){
  for(index in 1:ncol(normal)){
    plot(normal[, index], type = "l", col = "red", 
         ylim = c(min(min(normal[,index]), min(trunNorm[,index])), 
                  max(max(normal[,index]), max(trunNorm[,index]))), 
                  main = colnames(trunNorm)[index])
    lines(trunNorm[, index], col = "black")}
  }
  
  else{
    for(index in 1:ncol(normal)){
      plot(normal[burnin:ng, index], type = "l", col = "red", 
           ylim = c(min(min(normal[burnin:ng,index]), min(trunNorm[burnin:ng,index])), 
                    max(max(normal[burnin:ng,index]), max(trunNorm[burnin:ng,index]))),
           main = colnames(trunNorm)[index])
      lines(trunNorm[burnin:ng, index], col = "black")}
  }
}

```

## Exercise 1

```{r q1}
#----------------------------------------------------------------------------
## QUESTION 1

# Simulate Data
n  <- 10             
s  <- 1
mu <- -2
y  <- rnorm(n, mu, sqrt(s))

# Prior parameters
m  <- 0                    
M  <- 1
s1 <- s2 <- 1

# Chains
mg <- 0
sg <- 2
G <- 1000
chains <- matrix(NA,G,2)
for(g in 1:G){
  mg <- getMu(sg, y, M, m)
  sg <- getSigma(mg, y, s1, s2)
  chains[g,] <- c(mg, sg)
}
qc <- apply(chains,2,quantile,c(.025,.975))

# Summarize and output
summ <- data.frame(mean = colMeans(chains), 
                   median = apply(chains,2,median), 
                   Quantile1 = apply(chains,2,quantile,c(.025,.975))[1,],
                   Quantile2 = apply(chains,2,quantile,c(.025,.975))[2,],
                   cor_Mean = cor(chains)[1,],
                   cor_Variance = cor(chains)[2,])
row.names(summ)[1] <- c("Mean")
row.names(summ)[2] <- c("Variance")
knitr::kable( summ, caption = "Summary for Chains of Mean and Variance Parameter", digits=3)

```

## Exercise 2

Using a Metropolis algorithm (see [Appendix]{#appendix}), we found that the Metropolis lambda was reasonably close to the original lambda, at least if one were to round to the nearest whole number.


```{r q2} 
#----------------------------------------------------------------------------
## QUESTION 2

# Metropolis algorithm
n <- 100
nrep <- 1000
ntimes <- 10

# Let's run this a couple of times to compare.
Lcur.df <- data.frame(OriginalLambda = rep(0, ntimes),
                      MetropolisLambda = rep(0, ntimes), 
                      DistanceFromOriginal = rep(0,ntimes))
for (i in 1:ntimes) {
  Lcur <- runif(1, 0, 10)
  lambda <- runif(1, 0, 10)
  y <- rpois(n, lambda)
  
  # Loop over a number of repetitions
  for (j in 1:nrep) {
    # Propose a new lambda
    Lstar <- .tnorm(1, 0, 10, mu=Lcur, sig=2)
  
    # Find probability of each lambda given y
    num   <- prod(dpois(y, Lstar) * dunif(Lstar, 0, 10))
    denom <- prod(dpois(y, Lcur ) * dunif(Lcur, 0, 10))
    ratio <- num / denom
    
    # Accept or reject:
    if (!is.nan(ratio)) { #sometimes this is NaN for large nrep; not sure why
      u <- runif(1, 0, 1)
      if (u < ratio) Lcur <- Lstar
    }
  }
  
  # Save
  Lcur.df[i,] <- c(lambda, Lcur, abs(Lcur - lambda))
}

knitr::kable( Lcur.df, caption = "Summary for ten runs of Metropolis algorithm", digits=2)


# y
n <- 100
lambda <- 3
y <- rpois(n, lambda)

# proposed lambda 
L0 <- 8
sig <- 3  # variance for the proposal distribution
nrep <- 100
LAMBDA <- NULL
par(mfrow=c(2,3))

for(sig in c(0.05,0.5,1,3,6,10)){
  for (i in 1:nrep){
    Lstar <- .tnorm(1,0,10,L0,sig)
    r <- prod(dpois(y,Lstar)*dunif(Lstar, min = 0, max = 10))/prod(dpois(y,L0) *dunif(L0, min = 0, max = 10))
    
    # reject or accept 
    if (r > runif(1,0,1)) L0 <- Lstar
    LAMBDA <- c(LAMBDA, L0)
  }
  plot(density(LAMBDA), main = paste("sigma = ", sig), xlim = c(0,10))
  LAMBDA <- NULL
}

# The variance of the posterior distribution is lambda =3. As the figure shows, the proposal distribution with sigma = 3 is the most efficient one. 
```



## Exercise 3

While both chains converge, normal and truncated normal, the truncated 
normal proposal matrix is more efficient. Without the truncated normal, the 
MCMC tends to get stuck more often, which may cause issues with inference on
the posterior. 

```{r q3}
#----------------------------------------------------------------------------
## QUESTION 3

form  <- as.formula(cones ~ nfert*trt + diam)
X     <- model.matrix(form, data=treedata)
Y     <- model.frame(form, data=treedata)$cones
p     <- ncol(X)
n     <- nrow(X)

bg <- priorB <- matrix(0,p)
xnames <- colnames(X)
rownames(bg) <- xnames
priorVB <- diag(p)*1000
hi <- bg + 100                                # big enough to not matter
lo <- -hi
lo[xnames %in% c('nfert','trt','diam')] <- 0  # positive main effects
cmat    <- .1*solve(crossprod(X))

ng     <- 10000
bgibbs <- matrix(NA,ng,p)  #for regression parameters
colnames(bgibbs) <- colnames(X)
accept <- 0

cupdate <- c(200, 500, 1000, 2000)
accept  <- 0

for(g in 1:ng){
  
  tmp <- updateBetaGLM(bg, cmat, X, Y, likelihood='dpois',
                       link='log', priorB = priorB, priorVB = priorVB,
                       lo = lo, hi = hi)
  bgibbs[g,] <- bg <- tmp$beta
  accept     <- accept + tmp$accept
  
  if(g %in% cupdate){
    cmat <- .1*var(bgibbs[1:(g-1),])  #adapt proposal
    diag(cmat) <- diag(cmat)*1.001
  }
}

.processPars(bgibbs, CPLOT=T)

.processPars(bgibbs[5000:ng,], CPLOT=T)


################## without truncated normal 


#run this part again -----

bg <- priorB <- matrix(0,p)
xnames <- colnames(X)
rownames(bg) <- xnames
priorVB <- diag(p)*1000
hi <- bg + 100                                # big enough to not matter
lo <- -hi
lo[xnames %in% c('nfert','trt','diam')] <- 0  # positive main effects
cmat    <- .1*solve(crossprod(X))

lo <- NULL #set bounds to null
hi <- NULL 

ng     <- 10000
str2 <- matrix(NA,ng,p)  #for regression parameters
colnames(str2) <- paste0(colnames(X), "- Normal")
accept <- 0
cupdate <- c(200, 500, 1000, 2000)
accept  <- 0

for(g in 1:ng){
  
  tmp <- updateBetaGLM(bg, cmat, X, Y, likelihood='dpois',
                       link='log', priorB = priorB, priorVB = priorVB,
                       lo = NULL, hi = NULL)
  str2[g,] <- bg <- tmp$beta
  accept     <- accept + tmp$accept
  
  if(g %in% cupdate){
    cmat <- .1*var(str2[1:(g-1),])  #adapt proposal
    diag(cmat) <- diag(cmat)*1.001
  }
}
.processPars(str2, CPLOT=T)
.processPars(str2[5000:ng,], CPLOT=T)

plotFn() # this is with truncated normal proposal
plotFn(burnin = 5000) #this is with normal proposal 
```


## Exercise 4



```{r q4}
#----------------------------------------------------------------------------
## QUESTION 4

# Plot params
colT <- colorRampPalette( c('#e5f5e0', '#a1d99b', '#31a354') )
cols <- colT(10)

# Data params
popsizes <- rep(c(5, 10, 20),2) #population sizes
npop <- 20               #no. populations
p  <- 0.6                #frequency of allele a
f  <- c(-.2, .2)         #inbreeding coefficient

# Gibbs params
g1 <- g2 <- 1   #beta shape params
priorF   <- 0   #mean and sd for f
priorFSD <- 1
ng     <- 5000
nchain <- 10
nt     <- 500

# Beta chains
mydata <- list(n1=list(), n2=list(), n3=list())
for (i in 1:len(popsizes)) {
  
  if (i-4 < 0) {
    inb.tx <- "negative inbreeding"
    fg <- f[1]
  } else {
    inb.tx <- "positive inbreeding"
    fg <- f[2] #positive inbreeding
  }

  # Create data
  pr <- pmake(c(p ,fg))     #values for (Paa,Pab,Pbb)
  y <-  rmultinom(npop, popsizes[i], pr) 
  
  # Reset chains
  thin   <- round(seq(1, ng, length=nt))  
  pgibbs <- matrix(0,nt,nchain)
  fgibbs <- pgibbs
    
  # Run sampler
  for(j in 1:nchain){
    
    k <- 0                # count variable 
    pg <- rbeta(1,g1,g2)  # draw initial values from prior
    lg <- minf(pg)        # lower limit for f
    fg <- .tnorm(1,lg,1,priorF,priorFSD) # random draw for f
    
    for(g in 1:ng){
      pf <- update_pf()   #from clarkfunctions2020.R
      pg <- pf$pg
      fg <- pf$fg
      
      if(g %in% thin){
        k <- k + 1
        pgibbs[k,j] <- pg
        fgibbs[k,j] <- fg
      }
    }
  }
  
  # Graph of joint 
  pseq <- seq(.01,.99,length=100)
  df1 <- data.frame(run   = as.factor(melt(pgibbs)$Var2),
                   pgibbs = melt(pgibbs)$value, 
                   fgibbs = melt(fgibbs)$value)
  df2 <- data.frame(pseq,minf(pseq))
  p1 <- ggplot(data=df1, aes(x=pgibbs, y=fgibbs, color=run), alpha=0.5) + 
          geom_point() + ggtitle(paste0('Beta Chains for n=', popsizes[i], " with ", inb.tx)) +
          xlim(c(0,1)) + ylim(c(-1,1)) +
          xlab('p') + ylab('f') +
          geom_line(data=df2, aes(x=df2$pseq, y=df2$minf.pseq.), color='black') +
          scale_color_manual(values = cols)
  p2 <- as.vector(fgibbs[-c(1:100),]) #without burnin
  mydata[[i]] <- list(n=popsizes[i], 
                      p1=p1,
                      p2=p2,
                      y=y)
}

mydata[[1]]$p1
mydata[[4]]$p1

mydata[[2]]$p1
mydata[[5]]$p1

mydata[[3]]$p1
mydata[[6]]$p1

```


## Appendix {#appendix}

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```











